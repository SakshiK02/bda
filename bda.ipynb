{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000aa2b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6127943e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5169659d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f03066",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7624654",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d60f5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdd396b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a4492b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55675e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6496a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7def3160",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e960d6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1a0bfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5be756",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edbe839",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be9da58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed76e0ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e567b3f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c82977",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27173eb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14019f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77be2f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443b2f1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131bd535",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8c97f1a8",
   "metadata": {},
   "source": [
    "### Bloom Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1204dbc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Bloom filter bit array:\n",
      "[0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0]\n",
      "29 is not seen.\n"
     ]
    }
   ],
   "source": [
    "#bloom filter\n",
    "# Initialize a Bloom filter with 18 bits (all set to 0)\n",
    "bit_array_size = 18\n",
    "bit_array = [0] * bit_array_size\n",
    "\n",
    "# Define the two hash functions\n",
    "def hash1(x):\n",
    "    return (x + 1) % 17\n",
    "\n",
    "def hash2(x):\n",
    "    return (3 * x + 2) % 17\n",
    "\n",
    "# Insert a value into the Bloom filter\n",
    "def insert(value):\n",
    "    # Apply both hash functions and set the corresponding bits\n",
    "    pos1 = hash1(value) % bit_array_size\n",
    "    pos2 = hash2(value) % bit_array_size\n",
    "    bit_array[pos1] = 1\n",
    "    bit_array[pos2] = 1\n",
    "\n",
    "# Values to be inserted\n",
    "values = [15, 23, 65]\n",
    "\n",
    "# Insert all values into the Bloom filter\n",
    "for value in values:\n",
    "    insert(value)\n",
    "\n",
    "# Print the final state of the bit array\n",
    "print(\"Final Bloom filter bit array:\")\n",
    "print(bit_array)\n",
    "\n",
    "def is_probably_present(value):\n",
    "    pos1 = hash1(value) % bit_array_size\n",
    "    pos2 = hash2(value) % bit_array_size\n",
    "    return bit_array[pos1] and bit_array[pos2]\n",
    "\n",
    "# Check if number is in the filter\n",
    "if is_probably_present(29):\n",
    "    print(\"29 is seen.\")\n",
    "else:\n",
    "    print(\"29 is not seen.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95413112",
   "metadata": {},
   "source": [
    "### Flajolet Martin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bf34714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated number of distinct elements: 16\n"
     ]
    }
   ],
   "source": [
    "def flajolet_martin(stream):\n",
    "  R = 5 # as 2^R = 32\n",
    "  max_zeroes = 0\n",
    "  for x in stream:\n",
    "    hash_value = bin((3*x + 7) % 32)[2:].zfill(R) # [2:] is to remove \"0b\" and zfill pads with 0s\n",
    "    trailing_zeroes = len(hash_value) - len(hash_value.rstrip('0'))\n",
    "    max_zeroes = max(max_zeroes, trailing_zeroes)\n",
    "  return 2**max_zeroes\n",
    "\n",
    "# Example usage:\n",
    "stream = [3, 1, 4, 1, 5, 9, 2, 6, 5]\n",
    "estimated_distinct = flajolet_martin(stream)\n",
    "print(f\"Estimated number of distinct elements: {estimated_distinct}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff25b71",
   "metadata": {},
   "source": [
    "### Min Hashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f67c4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def k_shingles(text, k):\n",
    "    words = text.split()\n",
    "    shingles = set()\n",
    "    for i in range(len(words) - k + 1):\n",
    "        shingle = ' '.join(words[i:i + k])\n",
    "        shingles.add(shingle)\n",
    "    return shingles\n",
    "\n",
    "# Define the hash functions\n",
    "def hash_function_1(x):\n",
    "    return (3 * x + 5) % 526\n",
    "\n",
    "def hash_function_2(x):\n",
    "    return (7 * x + 4) % 526\n",
    "\n",
    "def hash_function_3(x):\n",
    "    return (3 * x + 1) % 526\n",
    "\n",
    "# File paths\n",
    "file_paths = [\n",
    "    \"one.txt\",\n",
    "    \"two.txt\",\n",
    "    \"three.txt\"\n",
    "]\n",
    "\n",
    "# Token size\n",
    "k = 5\n",
    "\n",
    "# Dictionary to hold shingles for each file\n",
    "shingle_dict = {}\n",
    "\n",
    "# Process each file\n",
    "for file_path in file_paths:\n",
    "    with open(file_path, 'r') as file:\n",
    "        content = file.read()\n",
    "\n",
    "    shingles = k_shingles(content, k)\n",
    "    shingle_dict[file_path] = shingles\n",
    "\n",
    "# Create a set of all unique shingles across all files\n",
    "all_shingles = list(set.union(*[set(shingles) for shingles in shingle_dict.values()]))\n",
    "\n",
    "# Initialize an incidence matrix\n",
    "incidence_matrix = pd.DataFrame(0, index=all_shingles, columns=file_paths)\n",
    "\n",
    "# Populate the incidence matrix\n",
    "for file_path, shingles in shingle_dict.items():\n",
    "    for shingle in shingles:\n",
    "        incidence_matrix.at[shingle, file_path] = 1\n",
    "\n",
    "# Function to compute signatures\n",
    "def compute_signature(incidence_matrix):\n",
    "    signatures = {file_path: [] for file_path in incidence_matrix.columns}\n",
    "\n",
    "    for shingle in incidence_matrix.index:\n",
    "        # Use the index of the shingle in the all_shingles list\n",
    "        index = list(incidence_matrix.index).index(shingle)\n",
    "\n",
    "        for file_path in incidence_matrix.columns:\n",
    "            # Calculate hash values\n",
    "            hash_value = [\n",
    "                hash_function_1(index),\n",
    "                hash_function_2(index),\n",
    "                hash_function_3(index)\n",
    "            ]\n",
    "            # Append hash values if the shingle is present in the file\n",
    "            if incidence_matrix.at[shingle, file_path] == 1:\n",
    "                signatures[file_path].append(hash_value)\n",
    "\n",
    "    # Find the minimum hash value for each hash function per file\n",
    "    final_signatures = {}\n",
    "    for file_path, values in signatures.items():\n",
    "        if values:  # If there are any values\n",
    "            final_signatures[file_path] = [\n",
    "                min(value[0] for value in values),  # Min for Hash1\n",
    "                min(value[1] for value in values),  # Min for Hash2\n",
    "                min(value[2] for value in values)   # Min for Hash3\n",
    "            ]\n",
    "        else:\n",
    "            final_signatures[file_path] = [None, None, None]  # Handle case with no shingles\n",
    "\n",
    "    return pd.DataFrame(final_signatures, index=['Hash1', 'Hash2', 'Hash3']).T\n",
    "\n",
    "# Compute signatures\n",
    "signature_matrix = compute_signature(incidence_matrix)\n",
    "\n",
    "# Display the incidence matrix and signature matrix\n",
    "print(\"Incidence Matrix:\")\n",
    "print(incidence_matrix)\n",
    "\n",
    "print(\"\\nSignature Matrix:\")\n",
    "print(signature_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51753279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Signature Matrix:\n",
      "[[1. 0. 0. 1.]\n",
      " [0. 0. 3. 1.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Data matrix: each row represents an element's membership in the sets S1, S2, S3, S4.\n",
    "data = np.array([\n",
    "    [1, 0, 0, 1],  # a\n",
    "    [0, 0, 1, 0],  # b\n",
    "    [0, 1, 0, 1],  # c\n",
    "    [1, 1, 0, 0],  # d\n",
    "    [0, 1, 1, 0]   # e\n",
    "])\n",
    "\n",
    "# Define the hash functions\n",
    "def h1(x):\n",
    "    return (x + 1) % 5\n",
    "\n",
    "def h2(x):\n",
    "    return (3 * x + 1) % 5\n",
    "\n",
    "# Number of hash functions and rows (elements)\n",
    "num_hashes = 2\n",
    "num_elements = data.shape[0]\n",
    "\n",
    "# Initialize the signature matrix with infinity values\n",
    "signature_matrix = np.full((num_hashes, data.shape[1]), np.inf)\n",
    "\n",
    "# Compute the signature matrix\n",
    "for row in range(num_elements):\n",
    "    hashes = [h1(row), h2(row)]\n",
    "    for col in range(data.shape[1]):\n",
    "        if data[row, col] == 1:  # If the element is present in the set\n",
    "            for i in range(num_hashes):\n",
    "                signature_matrix[i, col] = min(signature_matrix[i, col], hashes[i])\n",
    "\n",
    "# Display the resulting signature matrix\n",
    "print(\"Signature Matrix:\")\n",
    "print(signature_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed87c48",
   "metadata": {},
   "source": [
    "### KShingles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbec9480",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install kshingle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50cc3627",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Neural', 'networks,', 'or', 'artificial', 'neural'),\n",
       " ('networks,', 'or', 'artificial', 'neural', 'networks,'),\n",
       " ('or', 'artificial', 'neural', 'networks,', 'attempt'),\n",
       " ('artificial', 'neural', 'networks,', 'attempt', 'to'),\n",
       " ('neural', 'networks,', 'attempt', 'to', 'mimic'),\n",
       " ('networks,', 'attempt', 'to', 'mimic', 'the'),\n",
       " ('attempt', 'to', 'mimic', 'the', 'human'),\n",
       " ('to', 'mimic', 'the', 'human', 'brain'),\n",
       " ('mimic', 'the', 'human', 'brain', 'through'),\n",
       " ('the', 'human', 'brain', 'through', 'a'),\n",
       " ('human', 'brain', 'through', 'a', 'combination'),\n",
       " ('brain', 'through', 'a', 'combination', 'of'),\n",
       " ('through', 'a', 'combination', 'of', 'data'),\n",
       " ('a', 'combination', 'of', 'data', 'inputs,'),\n",
       " ('combination', 'of', 'data', 'inputs,', 'weights'),\n",
       " ('of', 'data', 'inputs,', 'weights', 'and'),\n",
       " ('data', 'inputs,', 'weights', 'and', 'biasâ€”all'),\n",
       " ('inputs,', 'weights', 'and', 'biasâ€”all', 'acting'),\n",
       " ('weights', 'and', 'biasâ€”all', 'acting', 'as'),\n",
       " ('and', 'biasâ€”all', 'acting', 'as', 'silicon'),\n",
       " ('biasâ€”all', 'acting', 'as', 'silicon', 'neurons.'),\n",
       " ('acting', 'as', 'silicon', 'neurons.', 'These'),\n",
       " ('as', 'silicon', 'neurons.', 'These', 'elements'),\n",
       " ('silicon', 'neurons.', 'These', 'elements', 'work'),\n",
       " ('neurons.', 'These', 'elements', 'work', 'together'),\n",
       " ('These', 'elements', 'work', 'together', 'to'),\n",
       " ('elements', 'work', 'together', 'to', 'accurately'),\n",
       " ('work', 'together', 'to', 'accurately', 'recognize,'),\n",
       " ('together', 'to', 'accurately', 'recognize,', 'classify'),\n",
       " ('to', 'accurately', 'recognize,', 'classify', 'and'),\n",
       " ('accurately', 'recognize,', 'classify', 'and', 'describe'),\n",
       " ('recognize,', 'classify', 'and', 'describe', 'objects'),\n",
       " ('classify', 'and', 'describe', 'objects', 'within'),\n",
       " ('and', 'describe', 'objects', 'within', 'the'),\n",
       " ('describe', 'objects', 'within', 'the', 'data.'),\n",
       " ('objects', 'within', 'the', 'data.', 'Deep'),\n",
       " ('within', 'the', 'data.', 'Deep', 'neural'),\n",
       " ('the', 'data.', 'Deep', 'neural', 'networks'),\n",
       " ('data.', 'Deep', 'neural', 'networks', 'consist'),\n",
       " ('Deep', 'neural', 'networks', 'consist', 'of'),\n",
       " ('neural', 'networks', 'consist', 'of', 'multiple'),\n",
       " ('networks', 'consist', 'of', 'multiple', 'layers'),\n",
       " ('consist', 'of', 'multiple', 'layers', 'of'),\n",
       " ('of', 'multiple', 'layers', 'of', 'interconnected'),\n",
       " ('multiple', 'layers', 'of', 'interconnected', 'nodes,'),\n",
       " ('layers', 'of', 'interconnected', 'nodes,', 'each'),\n",
       " ('of', 'interconnected', 'nodes,', 'each', 'building'),\n",
       " ('interconnected', 'nodes,', 'each', 'building', 'on'),\n",
       " ('nodes,', 'each', 'building', 'on', 'the'),\n",
       " ('each', 'building', 'on', 'the', 'previous'),\n",
       " ('building', 'on', 'the', 'previous', 'layer'),\n",
       " ('on', 'the', 'previous', 'layer', 'to'),\n",
       " ('the', 'previous', 'layer', 'to', 'refine'),\n",
       " ('previous', 'layer', 'to', 'refine', 'and'),\n",
       " ('layer', 'to', 'refine', 'and', 'optimize'),\n",
       " ('to', 'refine', 'and', 'optimize', 'the'),\n",
       " ('refine', 'and', 'optimize', 'the', 'prediction'),\n",
       " ('and', 'optimize', 'the', 'prediction', 'or'),\n",
       " ('optimize', 'the', 'prediction', 'or', 'categorization.'),\n",
       " ('the', 'prediction', 'or', 'categorization.', 'This'),\n",
       " ('prediction', 'or', 'categorization.', 'This', 'progression'),\n",
       " ('or', 'categorization.', 'This', 'progression', 'of'),\n",
       " ('categorization.', 'This', 'progression', 'of', 'computations'),\n",
       " ('This', 'progression', 'of', 'computations', 'through'),\n",
       " ('progression', 'of', 'computations', 'through', 'the'),\n",
       " ('of', 'computations', 'through', 'the', 'network'),\n",
       " ('computations', 'through', 'the', 'network', 'is'),\n",
       " ('through', 'the', 'network', 'is', 'called'),\n",
       " ('the', 'network', 'is', 'called', 'forward'),\n",
       " ('network', 'is', 'called', 'forward', 'propagation.'),\n",
       " ('is', 'called', 'forward', 'propagation.', 'The'),\n",
       " ('called', 'forward', 'propagation.', 'The', 'input'),\n",
       " ('forward', 'propagation.', 'The', 'input', 'and'),\n",
       " ('propagation.', 'The', 'input', 'and', 'output'),\n",
       " ('The', 'input', 'and', 'output', 'layers'),\n",
       " ('input', 'and', 'output', 'layers', 'of'),\n",
       " ('and', 'output', 'layers', 'of', 'a'),\n",
       " ('output', 'layers', 'of', 'a', 'deep'),\n",
       " ('layers', 'of', 'a', 'deep', 'neural'),\n",
       " ('of', 'a', 'deep', 'neural', 'network'),\n",
       " ('a', 'deep', 'neural', 'network', 'are'),\n",
       " ('deep', 'neural', 'network', 'are', 'called'),\n",
       " ('neural', 'network', 'are', 'called', 'visible'),\n",
       " ('network', 'are', 'called', 'visible', 'layers.'),\n",
       " ('are', 'called', 'visible', 'layers.', 'The'),\n",
       " ('called', 'visible', 'layers.', 'The', 'input'),\n",
       " ('visible', 'layers.', 'The', 'input', 'layer'),\n",
       " ('layers.', 'The', 'input', 'layer', 'is'),\n",
       " ('The', 'input', 'layer', 'is', 'where'),\n",
       " ('input', 'layer', 'is', 'where', 'the'),\n",
       " ('layer', 'is', 'where', 'the', 'deep'),\n",
       " ('is', 'where', 'the', 'deep', 'learning'),\n",
       " ('where', 'the', 'deep', 'learning', 'model'),\n",
       " ('the', 'deep', 'learning', 'model', 'ingests'),\n",
       " ('deep', 'learning', 'model', 'ingests', 'the'),\n",
       " ('learning', 'model', 'ingests', 'the', 'data'),\n",
       " ('model', 'ingests', 'the', 'data', 'for'),\n",
       " ('ingests', 'the', 'data', 'for', 'processing,'),\n",
       " ('the', 'data', 'for', 'processing,', 'and'),\n",
       " ('data', 'for', 'processing,', 'and', 'the'),\n",
       " ('for', 'processing,', 'and', 'the', 'output'),\n",
       " ('processing,', 'and', 'the', 'output', 'layer'),\n",
       " ('and', 'the', 'output', 'layer', 'is'),\n",
       " ('the', 'output', 'layer', 'is', 'where'),\n",
       " ('output', 'layer', 'is', 'where', 'the'),\n",
       " ('layer', 'is', 'where', 'the', 'final'),\n",
       " ('is', 'where', 'the', 'final', 'prediction'),\n",
       " ('where', 'the', 'final', 'prediction', 'or'),\n",
       " ('the', 'final', 'prediction', 'or', 'classification'),\n",
       " ('final', 'prediction', 'or', 'classification', 'is'),\n",
       " ('prediction', 'or', 'classification', 'is', 'made.'),\n",
       " ('or', 'classification', 'is', 'made.', 'Another'),\n",
       " ('classification', 'is', 'made.', 'Another', 'process'),\n",
       " ('is', 'made.', 'Another', 'process', 'called'),\n",
       " ('made.', 'Another', 'process', 'called', 'backpropagation'),\n",
       " ('Another', 'process', 'called', 'backpropagation', 'uses'),\n",
       " ('process', 'called', 'backpropagation', 'uses', 'algorithms,'),\n",
       " ('called', 'backpropagation', 'uses', 'algorithms,', 'such'),\n",
       " ('backpropagation', 'uses', 'algorithms,', 'such', 'as'),\n",
       " ('uses', 'algorithms,', 'such', 'as', 'gradient'),\n",
       " ('algorithms,', 'such', 'as', 'gradient', 'descent,'),\n",
       " ('such', 'as', 'gradient', 'descent,', 'to'),\n",
       " ('as', 'gradient', 'descent,', 'to', 'calculate'),\n",
       " ('gradient', 'descent,', 'to', 'calculate', 'errors'),\n",
       " ('descent,', 'to', 'calculate', 'errors', 'in'),\n",
       " ('to', 'calculate', 'errors', 'in', 'predictions,'),\n",
       " ('calculate', 'errors', 'in', 'predictions,', 'and'),\n",
       " ('errors', 'in', 'predictions,', 'and', 'then'),\n",
       " ('in', 'predictions,', 'and', 'then', 'adjusts'),\n",
       " ('predictions,', 'and', 'then', 'adjusts', 'the'),\n",
       " ('and', 'then', 'adjusts', 'the', 'weights'),\n",
       " ('then', 'adjusts', 'the', 'weights', 'and'),\n",
       " ('adjusts', 'the', 'weights', 'and', 'biases'),\n",
       " ('the', 'weights', 'and', 'biases', 'of'),\n",
       " ('weights', 'and', 'biases', 'of', 'the'),\n",
       " ('and', 'biases', 'of', 'the', 'function'),\n",
       " ('biases', 'of', 'the', 'function', 'by'),\n",
       " ('of', 'the', 'function', 'by', 'moving'),\n",
       " ('the', 'function', 'by', 'moving', 'backwards'),\n",
       " ('function', 'by', 'moving', 'backwards', 'through'),\n",
       " ('by', 'moving', 'backwards', 'through', 'the'),\n",
       " ('moving', 'backwards', 'through', 'the', 'layers'),\n",
       " ('backwards', 'through', 'the', 'layers', 'to'),\n",
       " ('through', 'the', 'layers', 'to', 'train'),\n",
       " ('the', 'layers', 'to', 'train', 'the'),\n",
       " ('layers', 'to', 'train', 'the', 'model.'),\n",
       " ('to', 'train', 'the', 'model.', 'Together,'),\n",
       " ('train', 'the', 'model.', 'Together,', 'forward'),\n",
       " ('the', 'model.', 'Together,', 'forward', 'propagation'),\n",
       " ('model.', 'Together,', 'forward', 'propagation', 'and'),\n",
       " ('Together,', 'forward', 'propagation', 'and', 'backpropagation'),\n",
       " ('forward', 'propagation', 'and', 'backpropagation', 'enable'),\n",
       " ('propagation', 'and', 'backpropagation', 'enable', 'a'),\n",
       " ('and', 'backpropagation', 'enable', 'a', 'neural'),\n",
       " ('backpropagation', 'enable', 'a', 'neural', 'network'),\n",
       " ('enable', 'a', 'neural', 'network', 'to'),\n",
       " ('a', 'neural', 'network', 'to', 'make'),\n",
       " ('neural', 'network', 'to', 'make', 'predictions'),\n",
       " ('network', 'to', 'make', 'predictions', 'and'),\n",
       " ('to', 'make', 'predictions', 'and', 'correct'),\n",
       " ('make', 'predictions', 'and', 'correct', 'for'),\n",
       " ('predictions', 'and', 'correct', 'for', 'any'),\n",
       " ('and', 'correct', 'for', 'any', 'errors'),\n",
       " ('correct', 'for', 'any', 'errors', '.'),\n",
       " ('for', 'any', 'errors', '.', 'Over'),\n",
       " ('any', 'errors', '.', 'Over', 'time,'),\n",
       " ('errors', '.', 'Over', 'time,', 'the'),\n",
       " ('.', 'Over', 'time,', 'the', 'algorithm'),\n",
       " ('Over', 'time,', 'the', 'algorithm', 'becomes'),\n",
       " ('time,', 'the', 'algorithm', 'becomes', 'gradually'),\n",
       " ('the', 'algorithm', 'becomes', 'gradually', 'more'),\n",
       " ('algorithm', 'becomes', 'gradually', 'more', 'accurate.')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import deque\n",
    "\n",
    "def create_k_shingles(filepath, k):\n",
    "  shingles = []\n",
    "  current_shingle = deque(maxlen=k)\n",
    "  with open(filepath, 'r') as f:\n",
    "    for line in f:\n",
    "      tokens = line.strip().split()\n",
    "      for token in tokens:\n",
    "        current_shingle.append(token)\n",
    "        if len(current_shingle) == k:\n",
    "          shingles.append(tuple(current_shingle))\n",
    "  return shingles\n",
    "\n",
    "filepath = \"one.txt\"\n",
    "k = 5\n",
    "shingles = create_k_shingles(filepath, k)\n",
    "shingles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1b4d26",
   "metadata": {},
   "source": [
    "### AMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02c9741d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value at position 1 (1-based) is 3 and its count from position onward is 2\n",
      "Value at position 3 (1-based) is 4 and its count from position onward is 2\n",
      "Value at position 5 (1-based) is 3 and its count from position onward is 1\n",
      "The surprise number of the stream is: 21.0\n"
     ]
    }
   ],
   "source": [
    "def ams_algorithm(stream, x_values):\n",
    "    n = len(stream)\n",
    "    total_sum = 0\n",
    "\n",
    "    # Ensure x_values are within the range of the stream\n",
    "    valid_x_values = [x for x in x_values if 0 <= x <= n]\n",
    "\n",
    "    for index in valid_x_values:\n",
    "        # Convert 1-based index to 0-based index\n",
    "        zero_based_index = index - 1\n",
    "\n",
    "        # Access the value at the given index in the stream\n",
    "        x_j = stream[zero_based_index]\n",
    "\n",
    "        # Count occurrences of x_j in the stream from position `zero_based_index` onward\n",
    "        count = sum(1 for x in stream[zero_based_index:] if x == x_j)\n",
    "\n",
    "        # Print the count for debugging purposes\n",
    "        print(f\"Value at position {index} (1-based) is {x_j} and its count from position onward is {count}\")\n",
    "\n",
    "        # Calculate the contribution of this x_j to the estimate\n",
    "        estimate = n * (2 * count - 1)\n",
    "        total_sum += estimate\n",
    "\n",
    "    # Average the estimates from all x_values\n",
    "    if len(valid_x_values) > 0:\n",
    "        surprise_number = total_sum / len(valid_x_values)\n",
    "    else:\n",
    "        surprise_number = 0\n",
    "\n",
    "    return surprise_number\n",
    "\n",
    "# Example stream and x values\n",
    "stream = [3, 1, 4, 1, 3, 4, 2, 1, 2]\n",
    "x_values = [1, 3, 5]  # Given positions in the problem statement\n",
    "\n",
    "# Calculate the surprise number\n",
    "surprise_number = ams_algorithm(stream, x_values)\n",
    "\n",
    "print(f\"The surprise number of the stream is: {surprise_number}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa620d82",
   "metadata": {},
   "source": [
    "### Bipartite Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5d7814",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install networkx matplotlib\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "# Greedy algorithm for bipartite matching\n",
    "class BipartiteMatcher:\n",
    "    def __init__(self, U, V, edges):\n",
    "        # U and V are the sets of vertices in the bipartite graph\n",
    "        # edges is a list of tuples representing edges between U and V\n",
    "        self.U = U\n",
    "        self.V = V\n",
    "        self.edges = edges\n",
    "        self.matching = set()\n",
    "        self.matched_U = set()\n",
    "        self.matched_V = set()\n",
    "\n",
    "    def greedy_match(self):\n",
    "        for u, v in self.edges:\n",
    "            # Check if both u and v are not already matched\n",
    "            if u not in self.matched_U and v not in self.matched_V:\n",
    "                # Add the edge to the matching\n",
    "                self.matching.add((u, v))\n",
    "                # Mark both vertices as matched\n",
    "                self.matched_U.add(u)\n",
    "                self.matched_V.add(v)\n",
    "\n",
    "        return self.matching\n",
    "\n",
    "U = {1, 2, 3, 4}  # Set U\n",
    "V = {'a', 'b', 'c', 'd'}  # Set V\n",
    "\n",
    "# List of edges between U and V\n",
    "edges = [(1, 'a'), (2, 'b'), (3, 'a'), (4, 'c'), (2, 'd')]\n",
    "\n",
    "matcher = BipartiteMatcher(U, V, edges)\n",
    "matching = matcher.greedy_match()\n",
    "\n",
    "print(\"Maximal Matching:\", matching)\n",
    "\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming 'matching' is the output from your BipartiteMatcher\n",
    "matching = [(1, 'a'), (2, 'b'), (4, 'c')]  # Replace with your actual matching\n",
    "\n",
    "# Create a bipartite graph\n",
    "G = nx.Graph()\n",
    "G.add_nodes_from(U, bipartite=0)  # Add nodes from set U\n",
    "G.add_nodes_from(V, bipartite=1)  # Add nodes from set V\n",
    "G.add_edges_from(edges)  # Add all edges\n",
    "\n",
    "# Create a subgraph with only the matched edges\n",
    "matching_edges = list(matching)\n",
    "G_matching = G.edge_subgraph(matching_edges)\n",
    "\n",
    "# Draw the graph\n",
    "pos = nx.bipartite_layout(G, U)  # Separate U and V nodes for better visualization\n",
    "nx.draw(G, pos, with_labels=True, node_color=['lightblue' if node in U else 'lightgreen' for node in G.nodes()])\n",
    "nx.draw_networkx_edges(G_matching, pos, edge_color='red', width=2)  # Highlight matching edges\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7cba924",
   "metadata": {},
   "source": [
    "### Social Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8f2549",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pandas networkx\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "# Load the dataset from a CSV file\n",
    "# Assuming the CSV has two columns: 'source' and 'target' representing edges between nodes\n",
    "csv_file = 'Social_Network_Ads.csv'\n",
    "df = pd.read_csv(csv_file)\n",
    "df.head()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create an empty graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add edges from the CSV file to the graph\n",
    "# Ensure that your CSV contains 'source' and 'target' columns\n",
    "for index, row in df.iterrows():\n",
    "    G.add_edge(row['User ID'], row['EstimatedSalary'])\n",
    "\n",
    "# Calculate centrality measures\n",
    "degree_centrality = nx.degree_centrality(G)\n",
    "weighted_degree_centrality = {node: val * 100 for node, val in degree_centrality.items()}  # Placeholder for weighted degree\n",
    "pagerank = nx.pagerank(G)\n",
    "betweenness_centrality = nx.betweenness_centrality(G)\n",
    "\n",
    "# Sort the centralities by degree as in your image example\n",
    "degree_sorted = sorted(degree_centrality.items(), key=lambda x: x[1], reverse=True)\n",
    "weighted_sorted = sorted(weighted_degree_centrality.items(), key=lambda x: x[1], reverse=True)\n",
    "pagerank_sorted = sorted(pagerank.items(), key=lambda x: x[1], reverse=True)\n",
    "betweenness_sorted = sorted(betweenness_centrality.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Create a DataFrame to store centrality measures in a tabular format\n",
    "data = {\n",
    "    'Degree Name': [x[0] for x in degree_sorted],\n",
    "    'Degree Score': [round(x[1]*100) for x in degree_sorted],  # Rounded for display\n",
    "    'Weighted Degree Name': [x[0] for x in weighted_sorted],\n",
    "    'Weighted Degree Score': [round(x[1]) for x in weighted_sorted],  # Assumed weighted values\n",
    "    'Pagerank Name': [x[0] for x in pagerank_sorted],\n",
    "    'Pagerank Score': [round(x[1], 3) for x in pagerank_sorted],  # 3 decimal places\n",
    "    'Betweenness Name': [x[0] for x in betweenness_sorted],\n",
    "    'Betweenness Score': [round(x[1], 2) for x in betweenness_sorted],  # 2 decimal places\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df_centrality = pd.DataFrame(data)\n",
    "\n",
    "# Print the table\n",
    "print(df_centrality.to_string(index=False))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset from the CSV file (update the file path accordingly)\n",
    "file_path = 'Social_Network_Ads.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Normalize 'EstimatedSalary' to compare users\n",
    "df['EstimatedSalary_norm'] = (df['EstimatedSalary'] - df['EstimatedSalary'].mean()) / df['EstimatedSalary'].std()\n",
    "\n",
    "# Create a feature matrix with EstimatedSalary\n",
    "feature_matrix = df[['EstimatedSalary_norm']].values\n",
    "\n",
    "# Calculate similarity between users using cosine similarity\n",
    "similarity_matrix = cosine_similarity(feature_matrix)\n",
    "\n",
    "# Create a Graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add edges based on similarity (use a threshold to avoid connecting everyone)\n",
    "threshold = 0.8  # Only consider strong similarities\n",
    "for i, user1 in enumerate(df['User ID']):\n",
    "    for j, user2 in enumerate(df['User ID']):\n",
    "        if i != j and similarity_matrix[i, j] > threshold:\n",
    "            G.add_edge(user1, user2, weight=similarity_matrix[i, j])\n",
    "\n",
    "# Print basic graph info manually\n",
    "print(f\"Number of nodes: {G.number_of_nodes()}\")\n",
    "print(f\"Number of edges: {G.number_of_edges()}\")\n",
    "\n",
    "# Degree statistics\n",
    "degrees = dict(G.degree())\n",
    "print(f\"Average degree: {sum(degrees.values()) / len(degrees):.4f}\")\n",
    "\n",
    "# Subgraph for users with more than a certain number of connections\n",
    "subG = G.subgraph([n for n in G.nodes if G.degree(n) > 2])\n",
    "\n",
    "# Draw the subnetwork\n",
    "pos = nx.spring_layout(subG, weight='weight', iterations=20, k=4)\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.axis('off')\n",
    "plt.title('User Network based on Estimated Salary', fontsize=24)\n",
    "\n",
    "# Draw nodes\n",
    "node_sizes = [100 * subG.degree(node) ** 0.5 for node in subG.nodes()]\n",
    "nx.draw_networkx_nodes(subG, pos, node_size=node_sizes, node_color='#009fe3')\n",
    "\n",
    "# Draw edges with varying widths based on weight\n",
    "for e in subG.edges(data=True):\n",
    "    nx.draw_networkx_edges(subG, pos, edgelist=[e], width=e[2]['weight'] * 0.2, edge_color='#707070')  # Adjust width scaling if needed\n",
    "\n",
    "# Add labels\n",
    "nx.draw_networkx_labels(subG, pos, font_size=10)\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34d5294",
   "metadata": {},
   "source": [
    "### BFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0792c63f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load the Excel file\n",
    "data = pd.read_csv('Movie.csv')\n",
    "\n",
    "# Create a graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add nodes and edges from the Excel data\n",
    "for index, row in data.iterrows():\n",
    "    person = row['person']\n",
    "    movie = row['movie']  # Assuming your columns are named 'Name' and 'Movie'\n",
    "\n",
    "    # Add nodes for person and movie\n",
    "    G.add_node(person, type='person')\n",
    "    G.add_node(movie, type='movie')\n",
    "\n",
    "    # Add an edge between the person and the movie\n",
    "    G.add_edge(person, movie)\n",
    "\n",
    "# Color mapping based on node type\n",
    "color_map = {'person': 'skyblue', 'movie': 'orange'}\n",
    "\n",
    "# Assign colors\n",
    "node_colors = [color_map[data['type']] if data['type'] in color_map else 'lightgray' for node, data in G.nodes(data=True)]\n",
    "\n",
    "# Calculate node positions with a layout that spaces them out\n",
    "pos = nx.spring_layout(G, k=0.3, iterations=20)  # Adjust k for more space between nodes\n",
    "\n",
    "# Draw nodes with color\n",
    "nx.draw_networkx_nodes(G, pos, node_color=node_colors, node_size=700, alpha=0.8)\n",
    "\n",
    "# Draw edges\n",
    "nx.draw_networkx_edges(G, pos, edge_color='gray', alpha=0.5)\n",
    "\n",
    "# Labels\n",
    "nx.draw_networkx_labels(G, pos, font_size=10, font_color='black')\n",
    "\n",
    "# Display the graph\n",
    "plt.figure(figsize=(15, 15))  # Adjust the figure size as necessary\n",
    "plt.title('Network Graph of People and Movies')\n",
    "plt.axis('off')  # Turn off the axis\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e068e5",
   "metadata": {},
   "source": [
    "### PipeLine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fd9b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyspark\n",
    "!pip install findspark\n",
    "\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[1]\") \\\n",
    "                    .appName('SparkByExamples.com') \\\n",
    "                    .getOrCreate()\n",
    "\n",
    "data = [(\"James\",\"Smith\",\"USA\",\"CA\"),(\"Michael\",\"Rose\",\"USA\",\"NY\"), \\\n",
    "    (\"Robert\",\"Williams\",\"USA\",\"CA\"),(\"Maria\",\"Jones\",\"USA\",\"FL\") \\\n",
    "  ]\n",
    "columns=[\"firstname\",\"lastname\",\"country\",\"state\"]\n",
    "df=spark.createDataFrame(data=data,schema=columns)\n",
    "df.show()\n",
    "print(df.collect())\n",
    "\n",
    "states1=df.rdd.map(lambda x: x[3]).collect()\n",
    "print(states1)\n",
    "#['CA', 'NY', 'CA', 'FL']\n",
    "from collections import OrderedDict\n",
    "res = list(OrderedDict.fromkeys(states1))\n",
    "print(res)\n",
    "#['CA', 'NY', 'FL']\n",
    "\n",
    "\n",
    "#Example 2\n",
    "states2=df.rdd.map(lambda x: x.state).collect()\n",
    "print(states2)\n",
    "#['CA', 'NY', 'CA', 'FL']\n",
    "\n",
    "states3=df.select(df.state).collect()\n",
    "print(states3)\n",
    "#[Row(state='CA'), Row(state='NY'), Row(state='CA'), Row(state='FL')]\n",
    "\n",
    "states4=df.select(df.state).rdd.flatMap(lambda x: x).collect()\n",
    "print(states4)\n",
    "#['CA', 'NY', 'CA', 'FL']\n",
    "\n",
    "states5=df.select(df.state).toPandas()['state']\n",
    "states6=list(states5)\n",
    "print(states6)\n",
    "#['CA', 'NY', 'CA', 'FL']\n",
    "\n",
    "pandDF=df.select(df.state,df.firstname).toPandas()\n",
    "print(list(pandDF['state']))\n",
    "print(list(pandDF['firstname']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e43117",
   "metadata": {},
   "source": [
    "### Diabetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ccfa81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Piplene - Diabetes\n",
    "\n",
    "# Import necessary modules\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import Imputer, VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "import pyspark.sql.types as tp\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DiabetesPredictionPipeline\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "data = pd.read_csv('diabetes.csv')\n",
    "data\n",
    "data_replaced = data.replace(0, np.nan)\n",
    "data_replaced\n",
    "\n",
    "# Read the CSV file\n",
    "my_data = data_replaced\n",
    "\n",
    "# Define the schema for the data\n",
    "my_schema = tp.StructType([\n",
    "    tp.StructField(name='Pregnancies', dataType=tp.IntegerType(), nullable=True),\n",
    "    tp.StructField(name='Glucose', dataType=tp.IntegerType(), nullable=True),\n",
    "    tp.StructField(name='BloodPressure', dataType=tp.IntegerType(), nullable=True),\n",
    "    tp.StructField(name='SkinThickness', dataType=tp.IntegerType(), nullable=True),\n",
    "    tp.StructField(name='Insulin', dataType=tp.IntegerType(), nullable=True),\n",
    "    tp.StructField(name='BMI', dataType=tp.DoubleType(), nullable=True),\n",
    "    tp.StructField(name='DiabetesPedigreeFunction', dataType=tp.DoubleType(), nullable=True),\n",
    "    tp.StructField(name='Age', dataType=tp.IntegerType(), nullable=True),\n",
    "    tp.StructField(name='Outcome', dataType=tp.IntegerType(), nullable=True)\n",
    "])\n",
    "\n",
    "# Read the data again with the defined schema\n",
    "my_data = spark.read.csv('diabetes.csv', schema=my_schema, header=True)\n",
    "\n",
    "# Print the schema\n",
    "my_data.printSchema()\n",
    "\n",
    "# Define stages for the pipeline\n",
    "imputer = Imputer(\n",
    "    inputCols=my_data.columns,\n",
    "    outputCols=[\"{}_imputed\".format(c) for c in my_data.columns]\n",
    ").setStrategy(\"median\")\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness',\n",
    "               'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age'],\n",
    "    outputCol='features'\n",
    ")\n",
    "\n",
    "lr = LogisticRegression(featuresCol='features', labelCol='Outcome', maxIter=10)\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline(stages=[imputer, assembler, lr])\n",
    "\n",
    "# Split the data into training and test sets\n",
    "xtrain, xtest = my_data.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Fit the pipeline on training data\n",
    "pipeline_model = pipeline.fit(xtrain)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = pipeline_model.transform(xtest)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"Outcome\", predictionCol=\"prediction\")\n",
    "accuracy = evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"})\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef1987a",
   "metadata": {},
   "source": [
    "### PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7ee47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"CRUD Operations with TempView\").getOrCreate()\n",
    "\n",
    "# Manually creating a DataFrame using Row\n",
    "data = [\n",
    "    Row(ID=1, Name=\"Alice\", Age=25),\n",
    "    Row(ID=2, Name=\"Bob\", Age=30),\n",
    "    Row(ID=3, Name=\"Charlie\", Age=35)\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df_manual = spark.createDataFrame(data)\n",
    "\n",
    "# Create a temporary view for SQL queries\n",
    "df_manual.createOrReplaceTempView(\"people\")\n",
    "\n",
    "# Select records where Age is greater than 25\n",
    "filtered_records = spark.sql(\"SELECT * FROM people WHERE Age > 25\")\n",
    "filtered_records.show()\n",
    "\n",
    "# Insert new records\n",
    "new_data = [\n",
    "    Row(ID=4, Name=\"David\", Age=40),\n",
    "    Row(ID=5, Name=\"Eva\", Age=28)\n",
    "]\n",
    "df_new = spark.createDataFrame(new_data)\n",
    "df_combined = df_manual.union(df_new)\n",
    "df_combined.createOrReplaceTempView(\"people_combined\")\n",
    "spark.sql(\"SELECT * FROM people_combined\").show()\n",
    "\n",
    "# Update Age for the person with ID = 2\n",
    "updated_query = \"\"\"\n",
    "SELECT ID, Name,\n",
    "    CASE\n",
    "        WHEN ID = 2 THEN 32\n",
    "        ELSE Age\n",
    "    END AS Age\n",
    "FROM people_combined\n",
    "\"\"\"\n",
    "df_updated = spark.sql(updated_query)\n",
    "df_updated.createOrReplaceTempView(\"people_updated\")\n",
    "spark.sql(\"SELECT * FROM people_updated\").show()\n",
    "\n",
    "# Delete records where ID = 1\n",
    "df_deleted = spark.sql(\"SELECT * FROM people_updated WHERE ID != 1\")\n",
    "df_deleted.createOrReplaceTempView(\"people_updated\")\n",
    "spark.sql(\"SELECT * FROM people_updated\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f7d2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "states = {\"NY\":\"New York\", \"CA\":\"California\", \"FL\":\"Florida\"}\n",
    "broadcastStates = spark.sparkContext.broadcast(states)\n",
    "\n",
    "data = [(\"James\",\"Smith\",\"USA\",\"CA\"),\n",
    "    (\"Michael\",\"Rose\",\"USA\",\"NY\"),\n",
    "    (\"Robert\",\"Williams\",\"USA\",\"CA\"),\n",
    "    (\"Maria\",\"Jones\",\"USA\",\"FL\")\n",
    "  ]\n",
    "\n",
    "columns = [\"firstname\",\"lastname\",\"country\",\"state\"]\n",
    "df = spark.createDataFrame(data = data, schema = columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n",
    "\n",
    "def state_convert(code):\n",
    "    return broadcastStates.value[code]\n",
    "\n",
    "result = df.rdd.map(lambda x: (x[0],x[1],x[2],state_convert(x[3]))).toDF(columns)\n",
    "result.show(truncate=False)\n",
    "\n",
    "# Broadcast variable on filter\n",
    "\n",
    "filteDf= df.where((df['state'].isin(broadcastStates.value)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4852d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import DoubleType, IntegerType\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "          .appName('SparkByExamples.com') \\\n",
    "          .getOrCreate()\n",
    "\n",
    "simpleData = [(\"James\",\"34\",\"true\",\"M\",\"3000.6089\"),\n",
    "    (\"Michael\",\"33\",\"true\",\"F\",\"3300.8067\"),\n",
    "    (\"Robert\",\"37\",\"false\",\"M\",\"5000.5034\")\n",
    "  ]\n",
    "\n",
    "columns = [\"firstname\",\"age\",\"isGraduated\",\"gender\",\"salary\"]\n",
    "df = spark.createDataFrame(data = simpleData, schema = columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n",
    "\n",
    "from pyspark.sql.functions import col,round,expr\n",
    "df.withColumn(\"salary\",df.salary.cast('double')).printSchema()\n",
    "df.withColumn(\"salary\",df.salary.cast(DoubleType())).printSchema()\n",
    "df.withColumn(\"salary\",col(\"salary\").cast('double')).printSchema()\n",
    "\n",
    "#df.withColumn(\"salary\",round(df.salary.cast(DoubleType()),2)).show(truncate=False).printSchema()\n",
    "df.selectExpr(\"firstname\",\"isGraduated\",\"cast(salary as double) salary\").printSchema()\n",
    "\n",
    "df.createOrReplaceTempView(\"CastExample\")\n",
    "spark.sql(\"SELECT firstname,isGraduated,DOUBLE(salary) as salary from CastExample\").printSchema()\n",
    "\n",
    "\n",
    "#df.select(\"firstname\",expr(df.age),\"isGraduated\",col(\"salary\").cast('float').alias(\"salary\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea326fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "dept = [(\"Finance\",10), \\\n",
    "    (\"Marketing\",20), \\\n",
    "    (\"Sales\",30), \\\n",
    "    (\"IT\",40) \\\n",
    "  ]\n",
    "deptColumns = [\"dept_name\",\"dept_id\"]\n",
    "deptDF = spark.createDataFrame(data=dept, schema = deptColumns)\n",
    "deptDF.printSchema()\n",
    "deptDF.show(truncate=False)\n",
    "\n",
    "dataCollect = deptDF.collect()\n",
    "\n",
    "print(dataCollect)\n",
    "\n",
    "dataCollect2 = deptDF.select(\"dept_name\").collect()\n",
    "print(dataCollect2)\n",
    "\n",
    "for row in dataCollect:\n",
    "    print(row['dept_name'] + \",\" +str(row['dept_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6c9907",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "data=[(\"James\",\"Bond\",\"100\",None),\n",
    "      (\"Ann\",\"Varsa\",\"200\",'F'),\n",
    "      (\"Tom Cruise\",\"XXX\",\"400\",''),\n",
    "      (\"Tom Brand\",None,\"400\",'M')]\n",
    "columns=[\"fname\",\"lname\",\"id\",\"gender\"]\n",
    "df=spark.createDataFrame(data,columns)\n",
    "\n",
    "#alias\n",
    "from pyspark.sql.functions import expr\n",
    "df.select(df.fname.alias(\"first_name\"), \\\n",
    "          df.lname.alias(\"last_name\"), \\\n",
    "          expr(\" fname ||','|| lname\").alias(\"fullName\") \\\n",
    "   ).show()\n",
    "\n",
    "#asc, desc\n",
    "df.sort(df.fname.asc()).show()\n",
    "df.sort(df.fname.desc()).show()\n",
    "\n",
    "#cast\n",
    "df.select(df.fname,df.id.cast(\"int\")).printSchema()\n",
    "\n",
    "#between\n",
    "df.filter(df.id.between(100,300)).show()\n",
    "\n",
    "#contains\n",
    "df.filter(df.fname.contains(\"Cruise\")).show()\n",
    "\n",
    "#startswith, endswith()\n",
    "df.filter(df.fname.startswith(\"T\")).show()\n",
    "df.filter(df.fname.endswith(\"Cruise\")).show()\n",
    "\n",
    "#eqNullSafe\n",
    "\n",
    "#isNull & isNotNull\n",
    "df.filter(df.lname.isNull()).show()\n",
    "df.filter(df.lname.isNotNull()).show()\n",
    "\n",
    "#like , rlike\n",
    "df.select(df.fname,df.lname,df.id) \\\n",
    "  .filter(df.fname.like(\"%om\"))\n",
    "\n",
    "#over\n",
    "\n",
    "#substr\n",
    "df.select(df.fname.substr(1,2).alias(\"substr\")).show()\n",
    "\n",
    "#when & otherwise\n",
    "from pyspark.sql.functions import when\n",
    "df.select(df.fname,df.lname,when(df.gender==\"M\",\"Male\") \\\n",
    "              .when(df.gender==\"F\",\"Female\") \\\n",
    "              .when(df.gender==None ,\"\") \\\n",
    "              .otherwise(df.gender).alias(\"new_gender\") \\\n",
    "    ).show()\n",
    "\n",
    "#isin\n",
    "li=[\"100\",\"200\"]\n",
    "df.select(df.fname,df.lname,df.id) \\\n",
    "  .filter(df.id.isin(li)) \\\n",
    "  .show()\n",
    "\n",
    "from pyspark.sql.types import StructType,StructField,StringType,ArrayType,MapType\n",
    "data=[((\"James\",\"Bond\"),[\"Java\",\"C#\"],{'hair':'black','eye':'brown'}),\n",
    "      ((\"Ann\",\"Varsa\"),[\".NET\",\"Python\"],{'hair':'brown','eye':'black'}),\n",
    "      ((\"Tom Cruise\",\"\"),[\"Python\",\"Scala\"],{'hair':'red','eye':'grey'}),\n",
    "      ((\"Tom Brand\",None),[\"Perl\",\"Ruby\"],{'hair':'black','eye':'blue'})]\n",
    "\n",
    "schema = StructType([\n",
    "        StructField('name', StructType([\n",
    "            StructField('fname', StringType(), True),\n",
    "            StructField('lname', StringType(), True)])),\n",
    "        StructField('languages', ArrayType(StringType()),True),\n",
    "        StructField('properties', MapType(StringType(),StringType()),True)\n",
    "     ])\n",
    "df=spark.createDataFrame(data,schema)\n",
    "df.printSchema()\n",
    "#getItem()\n",
    "df.select(df.languages.getItem(1)).show()\n",
    "\n",
    "df.select(df.properties.getItem(\"hair\")).show()\n",
    "\n",
    "#getField from Struct or Map\n",
    "df.select(df.properties.getField(\"hair\")).show()\n",
    "\n",
    "df.select(df.name.getField(\"fname\")).show()\n",
    "\n",
    "#dropFields\n",
    "#from pyspark.sql.functions import col\n",
    "#df.withColumn(\"name1\",col(\"name\").dropFields([\"fname\"])).show()\n",
    "\n",
    "#withField\n",
    "#from pyspark.sql.functions import lit\n",
    "#df.withColumn(\"name\",df.name.withField(\"fname\",lit(\"AA\"))).show()\n",
    "\n",
    "#from pyspark.sql import Row\n",
    "#from pyspark.sql.functions import lit\n",
    "#df = spark.createDataFrame([Row(a=Row(b=1, c=2))])\n",
    "#df.withColumn('a', df['a'].withField('b', lit(3))).select('a.b').show()\n",
    "\n",
    "#from pyspark.sql import Row\n",
    "#from pyspark.sql.functions import col, lit\n",
    "#df = spark.createDataFrame([\n",
    "#Row(a=Row(b=1, c=2, d=3, e=Row(f=4, g=5, h=6)))])\n",
    "#df.withColumn('a', df['a'].dropFields('b')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9c9e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession,Row\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "data=[(\"James\",23),(\"Ann\",40)]\n",
    "df=spark.createDataFrame(data).toDF(\"name.fname\",\"gender\")\n",
    "df.printSchema()\n",
    "df.show()\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "df.select(col(\"`name.fname`\")).show()\n",
    "df.select(df[\"`name.fname`\"]).show()\n",
    "df.withColumn(\"new_col\",col(\"`name.fname`\").substr(1,2)).show()\n",
    "df.filter(col(\"`name.fname`\").startswith(\"J\")).show()\n",
    "new_cols=(column.replace('.', '_') for column in df.columns)\n",
    "df2 = df.toDF(*new_cols)\n",
    "df2.show()\n",
    "\n",
    "\n",
    "# Using DataFrame object\n",
    "df.select(df.gender).show()\n",
    "df.select(df[\"gender\"]).show()\n",
    "#Accessing column name with dot (with backticks)\n",
    "df.select(df[\"`name.fname`\"]).show()\n",
    "\n",
    "#Using SQL col() function\n",
    "from pyspark.sql.functions import col\n",
    "df.select(col(\"gender\")).show()\n",
    "#Accessing column name with dot (with backticks)\n",
    "df.select(col(\"`name.fname`\")).show()\n",
    "\n",
    "#Access struct column\n",
    "data=[Row(name=\"James\",prop=Row(hair=\"black\",eye=\"blue\")),\n",
    "      Row(name=\"Ann\",prop=Row(hair=\"grey\",eye=\"black\"))]\n",
    "df=spark.createDataFrame(data)\n",
    "df.printSchema()\n",
    "\n",
    "df.select(df.prop.hair).show()\n",
    "df.select(df[\"prop.hair\"]).show()\n",
    "df.select(col(\"prop.hair\")).show()\n",
    "df.select(col(\"prop.*\")).show()\n",
    "\n",
    "# Column operators\n",
    "data=[(100,2,1),(200,3,4),(300,4,4)]\n",
    "df=spark.createDataFrame(data).toDF(\"col1\",\"col2\",\"col3\")\n",
    "df.select(df.col1 + df.col2).show()\n",
    "df.select(df.col1 - df.col2).show()\n",
    "df.select(df.col1 * df.col2).show()\n",
    "df.select(df.col1 / df.col2).show()\n",
    "df.select(df.col1 % df.col2).show()\n",
    "\n",
    "df.select(df.col2 > df.col3).show()\n",
    "df.select(df.col2 < df.col3).show()\n",
    "df.select(df.col2 == df.col3).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45e80e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "dataDictionary = [\n",
    "        ('James',{'hair':'black','eye':'brown'}),\n",
    "        ('Michael',{'hair':'brown','eye':None}),\n",
    "        ('Robert',{'hair':'red','eye':'black'}),\n",
    "        ('Washington',{'hair':'grey','eye':'grey'}),\n",
    "        ('Jefferson',{'hair':'brown','eye':''})\n",
    "        ]\n",
    "\n",
    "df = spark.createDataFrame(data=dataDictionary, schema = ['name','properties'])\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n",
    "\n",
    "df3=df.rdd.map(lambda x: \\\n",
    "    (x.name,x.properties[\"hair\"],x.properties[\"eye\"])) \\\n",
    "    .toDF([\"name\",\"hair\",\"eye\"])\n",
    "df3.printSchema()\n",
    "df3.show()\n",
    "\n",
    "df.withColumn(\"hair\",df.properties.getItem(\"hair\")) \\\n",
    "  .withColumn(\"eye\",df.properties.getItem(\"eye\")) \\\n",
    "  .drop(\"properties\") \\\n",
    "  .show()\n",
    "\n",
    "df.withColumn(\"hair\",df.properties[\"hair\"]) \\\n",
    "  .withColumn(\"eye\",df.properties[\"eye\"]) \\\n",
    "  .drop(\"properties\") \\\n",
    "  .show()\n",
    "\n",
    "# Functions\n",
    "from pyspark.sql.functions import explode,map_keys,col\n",
    "keysDF = df.select(explode(map_keys(df.properties))).distinct()\n",
    "keysList = keysDF.rdd.map(lambda x:x[0]).collect()\n",
    "keyCols = list(map(lambda x: col(\"properties\").getItem(x).alias(str(x)), keysList))\n",
    "df.select(df.name, *keyCols).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbf76f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.types import StructType,StructField, StringType\n",
    "\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "#Using List\n",
    "dept = [(\"Finance\",10),\n",
    "        (\"Marketing\",20),\n",
    "        (\"Sales\",30)\n",
    "      ]\n",
    "\n",
    "deptColumns = [\"dept_name\",\"dept_id\"]\n",
    "deptDF = spark.createDataFrame(data=dept, schema = deptColumns)\n",
    "deptDF.printSchema()\n",
    "deptDF.show(truncate=False)\n",
    "\n",
    "deptSchema = StructType([\n",
    "    StructField('firstname', StringType()),\n",
    "    StructField('middlename', StringType())\n",
    "])\n",
    "\n",
    "deptDF1 = spark.createDataFrame(data=dept, schema = deptSchema)\n",
    "deptDF1.printSchema()\n",
    "deptDF1.show(truncate=False)\n",
    "\n",
    "# Using list of Row type\n",
    "dept2 = [Row(\"Finance\",10),\n",
    "        Row(\"Marketing\",20),\n",
    "        Row(\"Sales\",30),\n",
    "        Row(\"IT\",40)\n",
    "      ]\n",
    "\n",
    "deptDF2 = spark.createDataFrame(data=dept, schema = deptColumns)\n",
    "deptDF2.printSchema()\n",
    "deptDF2.show(truncate=False)\n",
    "\n",
    "# Convert list to RDD\n",
    "rdd = spark.sparkContext.parallelize(dept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d32cc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com') \\\n",
    "        .master(\"local[5]\").getOrCreate()\n",
    "\n",
    "df=spark.range(0,20)\n",
    "print(df.rdd.getNumPartitions())\n",
    "\n",
    "df.write.mode(\"overwrite\").csv(\"c:/tmp/partition.csv\")\n",
    "\n",
    "df2 = df.repartition(6)\n",
    "print(df2.rdd.getNumPartitions())\n",
    "\n",
    "df3 = df.coalesce(2)\n",
    "print(df3.rdd.getNumPartitions())\n",
    "\n",
    "df4 = df.groupBy(\"id\").count()\n",
    "print(df4.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e36bc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "               .appName('SparkByExamples.com') \\\n",
    "               .getOrCreate()\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "df=spark.createDataFrame([[\"1\"]],[\"id\"])\n",
    "df.select(current_date().alias(\"current_date\"), \\\n",
    "      date_format(current_date(),\"yyyy MM dd\").alias(\"yyyy MM dd\"), \\\n",
    "      date_format(current_timestamp(),\"MM/dd/yyyy hh:mm\").alias(\"MM/dd/yyyy\"), \\\n",
    "      date_format(current_timestamp(),\"yyyy MMM dd\").alias(\"yyyy MMMM dd\"), \\\n",
    "      date_format(current_timestamp(),\"yyyy MMMM dd E\").alias(\"yyyy MMMM dd E\") \\\n",
    "   ).show()\n",
    "\n",
    "#SQL\n",
    "\n",
    "spark.sql(\"select current_date() as current_date, \"+\n",
    "      \"date_format(current_timestamp(),'yyyy MM dd') as yyyy_MM_dd, \"+\n",
    "      \"date_format(current_timestamp(),'MM/dd/yyyy hh:mm') as MM_dd_yyyy, \"+\n",
    "      \"date_format(current_timestamp(),'yyyy MMM dd') as yyyy_MMMM_dd, \"+\n",
    "      \"date_format(current_timestamp(),'yyyy MMMM dd E') as yyyy_MMMM_dd_E\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23dad57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "               .appName('SparkByExamples.com') \\\n",
    "               .getOrCreate()\n",
    "data=[[\"1\",\"2020-02-01\"],[\"2\",\"2019-03-01\"],[\"3\",\"2021-03-01\"]]\n",
    "df=spark.createDataFrame(data,[\"id\",\"input\"])\n",
    "df.show()\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "#current_date()\n",
    "df.select(current_date().alias(\"current_date\")\n",
    "  ).show(1)\n",
    "\n",
    "#date_format()\n",
    "df.select(col(\"input\"),\n",
    "    date_format(col(\"input\"), \"MM-dd-yyyy\").alias(\"date_format\")\n",
    "  ).show()\n",
    "\n",
    "#to_date()\n",
    "df.select(col(\"input\"),\n",
    "    to_date(col(\"input\"), \"yyy-MM-dd\").alias(\"to_date\")\n",
    "  ).show()\n",
    "\n",
    "#datediff()\n",
    "df.select(col(\"input\"),\n",
    "    datediff(current_date(),col(\"input\")).alias(\"datediff\")\n",
    "  ).show()\n",
    "\n",
    "#months_between()\n",
    "df.select(col(\"input\"),\n",
    "    months_between(current_date(),col(\"input\")).alias(\"months_between\")\n",
    "  ).show()\n",
    "\n",
    "#trunc()\n",
    "df.select(col(\"input\"),\n",
    "    trunc(col(\"input\"),\"Month\").alias(\"Month_Trunc\"),\n",
    "    trunc(col(\"input\"),\"Year\").alias(\"Month_Year\"),\n",
    "    trunc(col(\"input\"),\"Month\").alias(\"Month_Trunc\")\n",
    "   ).show()\n",
    "\n",
    "#add_months() , date_add(), date_sub()\n",
    "\n",
    "df.select(col(\"input\"),\n",
    "    add_months(col(\"input\"),3).alias(\"add_months\"),\n",
    "    add_months(col(\"input\"),-3).alias(\"sub_months\"),\n",
    "    date_add(col(\"input\"),4).alias(\"date_add\"),\n",
    "    date_sub(col(\"input\"),4).alias(\"date_sub\")\n",
    "  ).show()\n",
    "\n",
    "#\n",
    "\n",
    "df.select(col(\"input\"),\n",
    "     year(col(\"input\")).alias(\"year\"),\n",
    "     month(col(\"input\")).alias(\"month\"),\n",
    "     next_day(col(\"input\"),\"Sunday\").alias(\"next_day\"),\n",
    "     weekofyear(col(\"input\")).alias(\"weekofyear\")\n",
    "  ).show()\n",
    "\n",
    "df.select(col(\"input\"),\n",
    "     dayofweek(col(\"input\")).alias(\"dayofweek\"),\n",
    "     dayofmonth(col(\"input\")).alias(\"dayofmonth\"),\n",
    "     dayofyear(col(\"input\")).alias(\"dayofyear\"),\n",
    "  ).show()\n",
    "\n",
    "data=[[\"1\",\"02-01-2020 11 01 19 06\"],[\"2\",\"03-01-2019 12 01 19 406\"],[\"3\",\"03-01-2021 12 01 19 406\"]]\n",
    "df2=spark.createDataFrame(data,[\"id\",\"input\"])\n",
    "df2.show(truncate=False)\n",
    "\n",
    "#current_timestamp()\n",
    "df2.select(current_timestamp().alias(\"current_timestamp\")\n",
    "  ).show(1,truncate=False)\n",
    "\n",
    "#to_timestamp()\n",
    "df2.select(col(\"input\"),\n",
    "    to_timestamp(col(\"input\"), \"MM-dd-yyyy HH mm ss SSS\").alias(\"to_timestamp\")\n",
    "  ).show(truncate=False)\n",
    "\n",
    "\n",
    "#hour, minute,second\n",
    "data=[[\"1\",\"2020-02-01 11:01:19.06\"],[\"2\",\"2019-03-01 12:01:19.406\"],[\"3\",\"2021-03-01 12:01:19.406\"]]\n",
    "df3=spark.createDataFrame(data,[\"id\",\"input\"])\n",
    "\n",
    "df3.select(col(\"input\"),\n",
    "    hour(col(\"input\")).alias(\"hour\"),\n",
    "    minute(col(\"input\")).alias(\"minute\"),\n",
    "    second(col(\"input\")).alias(\"second\")\n",
    "  ).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78341288",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "simpleData = ((\"James\",\"\",\"Smith\",\"36636\",\"NewYork\",3100), \\\n",
    "    (\"Michael\",\"Rose\",\"\",\"40288\",\"California\",4300), \\\n",
    "    (\"Robert\",\"\",\"Williams\",\"42114\",\"Florida\",1400), \\\n",
    "    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"Florida\",5500), \\\n",
    "    (\"Jen\",\"Mary\",\"Brown\",\"34561\",\"NewYork\",3000) \\\n",
    "  )\n",
    "columns= [\"firstname\",\"middlename\",\"lastname\",\"id\",\"location\",\"salary\"]\n",
    "\n",
    "df = spark.createDataFrame(data = simpleData, schema = columns)\n",
    "\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n",
    "\n",
    "df.drop(\"firstname\") \\\n",
    "  .printSchema()\n",
    "\n",
    "df.drop(col(\"firstname\")) \\\n",
    "  .printSchema()\n",
    "\n",
    "df.drop(df.firstname) \\\n",
    "  .printSchema()\n",
    "\n",
    "df.drop(\"firstname\",\"middlename\",\"lastname\") \\\n",
    "    .printSchema()\n",
    "\n",
    "cols = (\"firstname\",\"middlename\",\"lastname\")\n",
    "\n",
    "df.drop(*cols) \\\n",
    "   .printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b4186a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Set the SPARK_HOME environment variable\n",
    "os.environ['SPARK_HOME'] = '/path/to/your/spark/installation'\n",
    "# Example: os.environ['SPARK_HOME'] = '/opt/spark'\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[1]\") \\\n",
    "                    .appName('SparkByExamples.com') \\\n",
    "                    .getOrCreate()\n",
    "\n",
    "data = [(\"James\",\"Smith\",\"USA\",\"CA\"),(\"Michael\",\"Rose\",\"USA\",\"NY\"), \\\n",
    "    (\"Robert\",\"Williams\",\"USA\",\"CA\"),(\"Maria\",\"Jones\",\"USA\",\"FL\") \\\n",
    "  ]\n",
    "columns=[\"firstname\",\"lastname\",\"country\",\"state\"]\n",
    "df=spark.createDataFrame(data=data,schema=columns)\n",
    "df.show()\n",
    "print(df.collect())\n",
    "\n",
    "states1=df.rdd.map(lambda x: x[3]).collect()\n",
    "print(states1)\n",
    "#['CA', 'NY', 'CA', 'FL']\n",
    "from collections import OrderedDict\n",
    "res = list(OrderedDict.fromkeys(states1))\n",
    "print(res)\n",
    "#['CA', 'NY', 'FL']\n",
    "\n",
    "\n",
    "#Example 2\n",
    "states2=df.rdd.map(lambda x: x.state).collect()\n",
    "print(states2)\n",
    "#['CA', 'NY', 'CA', 'FL']\n",
    "\n",
    "states3=df.select(df.state).collect()\n",
    "print(states3)\n",
    "#[Row(state='CA'), Row(state='NY'), Row(state='CA'), Row(state='FL')]\n",
    "\n",
    "states4=df.select(df.state).rdd.flatMap(lambda x: x).collect()\n",
    "print(states4)\n",
    "#['CA', 'NY', 'CA', 'FL']\n",
    "\n",
    "states5=df.select(df.state).toPandas()['state']\n",
    "states6=list(states5)\n",
    "print(states6)\n",
    "#['CA', 'NY', 'CA', 'FL']\n",
    "\n",
    "pandDF=df.select(df.state,df.firstname).toPandas()\n",
    "print(list(pandDF['state']))\n",
    "print(list(pandDF['firstname']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e78e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[1]\") \\\n",
    "                    .appName('SparkByExamples.com') \\\n",
    "                    .getOrCreate()\n",
    "\n",
    "data = [(\"James\",\"Smith\",\"USA\",\"CA\"),(\"Michael\",\"Rose\",\"USA\",\"NY\"), \\\n",
    "    (\"Robert\",\"Williams\",\"USA\",\"CA\"),(\"Maria\",\"Jones\",\"USA\",\"FL\") \\\n",
    "  ]\n",
    "columns=[\"firstname\",\"lastname\",\"country\",\"state\"]\n",
    "df=spark.createDataFrame(data=data,schema=columns)\n",
    "df.show()\n",
    "print(df.collect())\n",
    "\n",
    "states1=df.rdd.map(lambda x: x[3]).collect()\n",
    "print(states1)\n",
    "#['CA', 'NY', 'CA', 'FL']\n",
    "from collections import OrderedDict\n",
    "res = list(OrderedDict.fromkeys(states1))\n",
    "print(res)\n",
    "#['CA', 'NY', 'FL']\n",
    "\n",
    "\n",
    "#Example 2\n",
    "states2=df.rdd.map(lambda x: x.state).collect()\n",
    "print(states2)\n",
    "#['CA', 'NY', 'CA', 'FL']\n",
    "\n",
    "states3=df.select(df.state).collect()\n",
    "print(states3)\n",
    "#[Row(state='CA'), Row(state='NY'), Row(state='CA'), Row(state='FL')]\n",
    "\n",
    "states4=df.select(df.state).rdd.flatMap(lambda x: x).collect()\n",
    "print(states4)\n",
    "#['CA', 'NY', 'CA', 'FL']\n",
    "\n",
    "states5=df.select(df.state).toPandas()['state']\n",
    "states6=list(states5)\n",
    "print(states6)\n",
    "#['CA', 'NY', 'CA', 'FL']\n",
    "\n",
    "pandDF=df.select(df.state,df.firstname).toPandas()\n",
    "print(list(pandDF['state']))\n",
    "print(list(pandDF['firstname']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8606c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "spark: SparkSession = SparkSession.builder \\\n",
    "    .master(\"local[1]\") \\\n",
    "    .appName(\"SparkByExamples.com\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "data = [\n",
    "    (\"James\",None,\"M\"),\n",
    "    (\"Anna\",\"NY\",\"F\"),\n",
    "    (\"Julia\",None,None)\n",
    "]\n",
    "\n",
    "columns = [\"name\",\"state\",\"gender\"]\n",
    "df =spark.createDataFrame(data,columns)\n",
    "\n",
    "df.printSchema()\n",
    "df.show()\n",
    "\n",
    "df.filter(\"state is NULL\").show()\n",
    "df.filter(df.state.isNull()).show()\n",
    "df.filter(col(\"state\").isNull()).show()\n",
    "\n",
    "df.filter(\"state IS NULL AND gender IS NULL\").show()\n",
    "df.filter(df.state.isNull() & df.gender.isNull()).show()\n",
    "\n",
    "df.filter(\"state is not NULL\").show()\n",
    "df.filter(\"NOT state is NULL\").show()\n",
    "df.filter(df.state.isNotNull()).show()\n",
    "df.filter(col(\"state\").isNotNull()).show()\n",
    "df.na.drop(subset=[\"state\"]).show()\n",
    "\n",
    "df.createOrReplaceTempView(\"DATA\")\n",
    "spark.sql(\"SELECT * FROM DATA where STATE IS NULL\").show()\n",
    "spark.sql(\"SELECT * FROM DATA where STATE IS NULL AND GENDER IS NULL\").show()\n",
    "spark.sql(\"SELECT * FROM DATA where STATE IS NOT NULL\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2517ce",
   "metadata": {},
   "source": [
    "### Calculating nth moment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7697bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moment: -0.024489795918367325\n"
     ]
    }
   ],
   "source": [
    "class StreamMoment:\n",
    "    def __init__(self, n):\n",
    "        self.n = n      # n-th moment\n",
    "        self.count = 0   # Count of elements\n",
    "        self.mean = 0    # Running mean\n",
    "        self.moment_n = 0 # n-th moment\n",
    "\n",
    "    def update(self, value):\n",
    "        # Increment count of elements\n",
    "        self.count += 1\n",
    "\n",
    "        # Compute new mean\n",
    "        delta = value - self.mean\n",
    "        self.mean += delta / self.count\n",
    "\n",
    "        # Update the n-th moment\n",
    "        self.moment_n += (delta ** self.n) * ((self.count - 1) / self.count)\n",
    "\n",
    "    def get_moment(self):\n",
    "        # Return the n-th moment divided by the number of elements\n",
    "        return self.moment_n / self.count if self.count > 0 else 0\n",
    "\n",
    "# n-th moment:\n",
    "stream_moment = StreamMoment(n=1)\n",
    "\n",
    "# Simulating a stream of data\n",
    "data_stream = [1, 0, 0, 1, 1, 1, 0]\n",
    "for data in data_stream:\n",
    "    stream_moment.update(data)\n",
    "\n",
    "# Get the n-th moment\n",
    "print(f\"Moment: {stream_moment.get_moment()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eed45a6",
   "metadata": {},
   "source": [
    "### PCY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "325b9f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequent Items: {'4', '2', '1', '5', '3'}\n",
      "Frequent Pairs: {('2', '4'), ('4', '5'), ('1', '2'), ('1', '4')}\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "import hashlib\n",
    "def hash_pair(pair, num_buckets):\n",
    "    return int(hashlib.md5(str(pair).encode()).hexdigest(), 16) % num_buckets\n",
    "def count_items(transactions):\n",
    "    item_count = defaultdict(int)\n",
    "    for transaction in transactions:\n",
    "        for item in transaction:\n",
    "            item_count[item] += 1\n",
    "    return item_count\n",
    "def count_pairs(transactions, frequent_items, num_buckets):\n",
    "    bucket = [0] * num_buckets\n",
    "    pair_count = defaultdict(int)\n",
    "\n",
    "    for transaction in transactions:\n",
    "        transaction = [item for item in transaction if item in frequent_items]\n",
    "        for pair in combinations(transaction, 2):\n",
    "            bucket[hash_pair(pair, num_buckets)] += 1\n",
    "\n",
    "    for transaction in transactions:\n",
    "        transaction = [item for item in transaction if item in frequent_items]\n",
    "        for pair in combinations(transaction, 2):\n",
    "            if bucket[hash_pair(pair, num_buckets)] >= 2:  # If the bucket is frequent\n",
    "                pair_count[pair] += 1\n",
    "\n",
    "    return pair_count\n",
    "def pcy_algorithm(transactions, min_support, num_buckets):\n",
    "    # First pass: count individual items and hash pairs into buckets\n",
    "    item_count = count_items(transactions)\n",
    "    frequent_items = {item for item, count in item_count.items() if count >= min_support}\n",
    "\n",
    "    # Second pass: count pairs using the frequent items and buckets\n",
    "    pair_count = count_pairs(transactions, frequent_items, num_buckets)\n",
    "    frequent_pairs = {pair for pair, count in pair_count.items() if count >= min_support}\n",
    "\n",
    "    return frequent_items, frequent_pairs\n",
    "transactions = [\n",
    "    ['1','2','3'],\n",
    "    ['4', '5'],\n",
    "    ['1','4', '5'],\n",
    "    ['1', '2', '4'],\n",
    "    ['3','4' ,'5'],\n",
    "    ['2','4','5']\n",
    "]\n",
    "\n",
    "min_support = 2\n",
    "num_buckets = 5\n",
    "\n",
    "frequent_items, frequent_pairs = pcy_algorithm(transactions, min_support, num_buckets)\n",
    "\n",
    "print(\"Frequent Items:\", frequent_items)\n",
    "print(\"Frequent Pairs:\", frequent_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db02361",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "from collections import defaultdict, Counter\n",
    "import pandas as pd\n",
    "\n",
    "def hash_function(itemset, size):\n",
    "    \"\"\" Simple hash function to distribute itemsets over a hash table. \"\"\"\n",
    "    return hash(itemset) % size\n",
    "\n",
    "def get_frequent_itemsets(transactions, min_support, hash_size):\n",
    "    \"\"\" Generate frequent itemsets using the PCY algorithm. \"\"\"\n",
    "    # First pass: Count 1-itemsets and hash pairs\n",
    "    item_counts = Counter()\n",
    "    pair_counts = defaultdict(int)\n",
    "    hash_table = [0] * hash_size\n",
    "\n",
    "    # Count 1-itemsets and hash pairs\n",
    "    for transaction in transactions:\n",
    "        # Convert items to strings for consistent comparison\n",
    "        transaction = [str(item) for item in transaction]\n",
    "        for item in transaction:\n",
    "            item_counts[item] += 1\n",
    "        for itemset in combinations(sorted(transaction), 2): # Now sorting will work correctly\n",
    "            pair_counts[itemset] += 1\n",
    "            hash_index = hash_function(itemset, hash_size)\n",
    "            hash_table[hash_index] += 1\n",
    "\n",
    "    # Filter frequent 1-itemsets\n",
    "    frequent_items = {item for item, count in item_counts.items() if count >= min_support}\n",
    "\n",
    "    # Second pass: Use hash table to count candidate 2-itemsets\n",
    "    frequent_pairs = defaultdict(int)\n",
    "    for transaction in transactions:\n",
    "        # Convert items to strings for consistent comparison\n",
    "        transaction_items = [str(item) for item in transaction if str(item) in frequent_items]\n",
    "        for itemset in combinations(sorted(transaction_items), 2):\n",
    "            hash_index = hash_function(itemset, hash_size)\n",
    "            if hash_table[hash_index] >= min_support:\n",
    "                frequent_pairs[itemset] += 1\n",
    "\n",
    "    # Filter frequent 2-itemsets\n",
    "    frequent_pairs = {itemset: count for itemset, count in frequent_pairs.items() if count >= min_support}\n",
    "\n",
    "    return frequent_items, frequent_pairs\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    transactions = pd.read_csv(\"/content/trans.csv\")\n",
    "    transactions = transactions.values.tolist()\n",
    "\n",
    "    min_support = 2\n",
    "    hash_size = 10  # Size of hash table\n",
    "\n",
    "    frequent_items, frequent_pairs = get_frequent_itemsets(transactions, min_support, hash_size)\n",
    "\n",
    "    print(\"Frequent Itemsets:\", frequent_items)\n",
    "    print(\"Frequent Pairs:\", frequent_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e744c213",
   "metadata": {},
   "source": [
    "### Decaying Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55071674",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decaying_window(tags, alpha):\n",
    "    # Initialize an empty dictionary to store decaying counts\n",
    "    decaying_counts = {}\n",
    "\n",
    "    for tag in tags:\n",
    "        # Update decaying counts for all tags\n",
    "        for key in decaying_counts.keys():\n",
    "            decaying_counts[key] *= (1 - alpha)\n",
    "\n",
    "        # Increment the count for the current tag\n",
    "        if tag in decaying_counts:\n",
    "            decaying_counts[tag] += 1\n",
    "        else:\n",
    "            decaying_counts[tag] = 1\n",
    "\n",
    "        # Print the decaying counts after processing each tag\n",
    "        print(f\"After '{tag}': {decaying_counts}\")\n",
    "\n",
    "    return decaying_counts\n",
    "\n",
    "# Example sequence of Twitter tags\n",
    "tags = [\"fifa\", \"ipl\", \"fifa\", \"ipl\", \"ipl\", \"ipl\", \"fifa\"]\n",
    "alpha = 0.1  # Decay factor\n",
    "\n",
    "# Calculate decaying counts\n",
    "final_decaying_counts = decaying_window(tags, alpha)\n",
    "\n",
    "# Final decaying counts after processing all tags\n",
    "print(\"\\nFinal Decaying Counts:\", final_decaying_counts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
